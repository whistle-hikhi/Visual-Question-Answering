{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNSxqn7JKQt+2sRaFX1jNOf"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. CNN + LSTM"
      ],
      "metadata": {
        "id": "5wdA2kMlgUxX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "6vOYM-ALgYwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import spacy"
      ],
      "metadata": {
        "id": "uIHBYa9PgWz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split Train Val Test"
      ],
      "metadata": {
        "id": "cilEjR4QiDh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load train data\n",
        "train_data = []\n",
        "train_set_path = './vaq2.0.TrainImages.txt'\n",
        "\n",
        "with open(train_set_path, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "        temp = line.split('\\t')\n",
        "        qa = temp[1].split('?')\n",
        "\n",
        "        if len(qa) == 3:\n",
        "            answer = qa[2].strip()\n",
        "        else:\n",
        "            answer = qa[1].strip()\n",
        "\n",
        "        data_sample = {\n",
        "            'image_path': temp[0][:-2],\n",
        "            'question': qa[0] + '?',\n",
        "            'answer': answer\n",
        "        }\n",
        "        train_data.append(data_sample)\n",
        "\n",
        "# Load val data\n",
        "val_data = []\n",
        "val_set_path = './vaq2.0.DevImages.txt'\n",
        "\n",
        "with open(val_set_path, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "        temp = line.split('\\t')\n",
        "        qa = temp[1].split('?')\n",
        "\n",
        "        if len(qa) == 3:\n",
        "            answer = qa[2].strip()\n",
        "        else:\n",
        "            answer = qa[1].strip()\n",
        "\n",
        "        data_sample = {\n",
        "            'image_path': temp[0][:-2],\n",
        "            'question': qa[0] + '?',\n",
        "            'answer': answer\n",
        "        }\n",
        "        val_data.append(data_sample)\n",
        "\n",
        "# Load test data\n",
        "test_data = []\n",
        "test_set_path = './vaq2.0.TestImages.txt'\n",
        "\n",
        "with open(test_set_path, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "        temp = line.split('\\t')\n",
        "        qa = temp[1].split('?')\n",
        "        if len(qa) == 3:\n",
        "            answer = qa[2].strip()\n",
        "        else:\n",
        "            answer = qa[1].strip()\n",
        "\n",
        "        data_sample = {\n",
        "            'image_path': temp[0][:-2],\n",
        "            'question': qa[0] + '?',\n",
        "            'answer': answer\n",
        "        }\n",
        "        test_data.append(data_sample)"
      ],
      "metadata": {
        "id": "XLfeT8NUiFOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vocab build"
      ],
      "metadata": {
        "id": "p4pxLyaepRhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eng = spacy.load(\"en_core_web_sm\") # Load the English model to tokenize English text\n",
        "\n",
        "def get_tokens(data_iter):\n",
        "    for sample in data_iter:\n",
        "        question = sample['question']\n",
        "        yield [token.text for token in eng.tokenizer(question)]\n",
        "\n",
        "vocab = build_vocab_from_iterator(\n",
        "    get_tokens(train_data),\n",
        "    min_freq=2,\n",
        "    specials= ['<pad>', '<sos>', '<eos>', '<unk>'],\n",
        "    special_first=True\n",
        ")\n",
        "vocab.set_default_index(vocab['<unk>'])"
      ],
      "metadata": {
        "id": "xjm9zb3ZpVm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build dictionary mapping classes"
      ],
      "metadata": {
        "id": "VxLVyBlRpoTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classes = set([sample['answer'] for sample in train_data])\n",
        "classes_to_idx = {\n",
        "    cls_name: idx for idx, cls_name in enumerate(classes)\n",
        "}\n",
        "idx_to_classes = {\n",
        "    idx: cls_name for idx, cls_name in enumerate(classes)\n",
        "}"
      ],
      "metadata": {
        "id": "BVI9ARTxpTh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize function"
      ],
      "metadata": {
        "id": "iwXeDOGbp49g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(question, max_sequence_length):\n",
        "    tokens = [token.text for token in eng.tokenizer(question)]\n",
        "    sequence = [vocab[token] for token in tokens]\n",
        "    if len(sequence) < max_sequence_length:\n",
        "        sequence += [vocab['<pad>']] * (max_sequence_length-len(sequence))\n",
        "    else:\n",
        "        sequence = sequence[:max_sequence_length]\n",
        "    return sequence"
      ],
      "metadata": {
        "id": "c9-GCd0gp6hJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class pytorch dataset"
      ],
      "metadata": {
        "id": "gIs8sgiZqHKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VQADataset(Dataset):\n",
        "    def __init__(\n",
        "          self,\n",
        "          data,\n",
        "          classes_to_idx,\n",
        "          max_seq_len=30,\n",
        "          transform=None,\n",
        "          root_dir='/content/val2014-resised/'\n",
        "          ):\n",
        "        self.transform = transform\n",
        "        self.data = data\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.root_dir = root_dir\n",
        "        self.classes_to_idx = classes_to_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "    img_path = os.path.join(self.root_dir, self.data[index]['image_path'])\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    if self.transform:\n",
        "        img = self.transform(img)\n",
        "\n",
        "    question = self.data[index]['question']\n",
        "    question = tokenize(question, self.max_seq_len)\n",
        "    question = torch.tensor(question, dtype=torch.long)\n",
        "\n",
        "    label = self.data[index]['answer']\n",
        "    label = classes_to_idx[label]\n",
        "    label = torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "    return img, question, label"
      ],
      "metadata": {
        "id": "u_fTyxiIqKWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transforms"
      ],
      "metadata": {
        "id": "UXIZ9O-Gqw9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        " ])"
      ],
      "metadata": {
        "id": "NfdyI0UDqzi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Objects"
      ],
      "metadata": {
        "id": "wC4QvHR5q5E9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = VQADataset(\n",
        "    train_data,\n",
        "    classes_to_idx=classes_to_idx,\n",
        "    transform=transform)\n",
        "\n",
        "val_dataset = VQADataset(\n",
        "    val_data,\n",
        "    classes_to_idx=classes_to_idx,\n",
        "    transform=transform)\n",
        "\n",
        "test_dataset = VQADataset(\n",
        "    test_data,\n",
        "    classes_to_idx=classes_to_idx,\n",
        "    transform=transform)"
      ],
      "metadata": {
        "id": "mFkaUTTBq62m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataLoader"
      ],
      "metadata": {
        "id": "pnlGUXLGrCjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_batch_size = 128\n",
        "test_batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=train_batch_size,\n",
        "    shuffle=True)\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=test_batch_size,\n",
        "    shuffle=False)\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=test_batch_size,\n",
        "    shuffle=False)"
      ],
      "metadata": {
        "id": "e20XOUYmrD5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "jON9QTJ6rInx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VQAModel(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_classes,\n",
        "        img_model_name='resnet50',\n",
        "        embeddding_dim=300,\n",
        "        n_layers=2,\n",
        "        hidden_size=128,\n",
        "        dropout_prob=0.2\n",
        "        ):\n",
        "\n",
        "        super(VQAModel, self).__init__()\n",
        "        self.image_encoder = timm.create_model(\n",
        "        img_model_name,\n",
        "        pretrained=True,\n",
        "        num_classes=hidden_size\n",
        "        )\n",
        "\n",
        "        self.embedding = nn.Embedding(len(vocab), embeddding_dim)\n",
        "        self.lstm = nn.LSTM(\n",
        "        input_size=embeddding_dim,\n",
        "        hidden_size=hidden_size,\n",
        "        num_layers=n_layers,\n",
        "        batch_first=True,\n",
        "        bidirectional=True\n",
        "        )\n",
        "        self.layernorm = nn.LayerNorm(hidden_size * 2)\n",
        "        self.fc1 = nn.Linear(hidden_size * 3, 256)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.fc2 = nn.Linear(256, n_classes)\n",
        "\n",
        "    def forward(self, img, text):\n",
        "        img_features = self.image_encoder(img)\n",
        "\n",
        "        text_emb = self.embedding(text)\n",
        "        lstm_out, _ = self.lstm(text_emb)\n",
        "\n",
        "        lstm_out = lstm_out[:,-1, :]\n",
        "        lstm_out = self.layernorm(lstm_out)\n",
        "\n",
        "        combined = torch.cat((img_features, lstm_out), dim=1)\n",
        "        x = self.fc1(combined)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "AQqfy1JIrKIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_classes = len(classes)\n",
        "img_model_name = 'resnet50'\n",
        "hidden_size = 128\n",
        "n_layers = 1\n",
        "embeddding_dim = 128\n",
        "dropout_prob = 0.2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model = VQAModel(\n",
        "    n_classes=n_classes,\n",
        "    img_model_name=img_model_name,\n",
        "    embeddding_dim=embeddding_dim,\n",
        "    n_layers=n_layers,\n",
        "    hidden_size=hidden_size,\n",
        "    dropout_prob=dropout_prob\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "Zn5x9a-QrR8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and Evaluate"
      ],
      "metadata": {
        "id": "U0K7fMQBrU1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for image, question, labels in dataloader:\n",
        "            image, question, labels = image.to(device), question.to(device), labels.to(device)\n",
        "            outputs = model(image, question)\n",
        "            loss = criterion(outputs, labels)\n",
        "            losses.append(loss.item())\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    loss = sum(losses) / len(losses)\n",
        "    acc = correct / total\n",
        "\n",
        "    return loss, acc\n",
        "\n",
        "def fit(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    device,\n",
        "    epochs\n",
        "    ):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        batch_train_losses = []\n",
        "\n",
        "        model.train()\n",
        "        for idx, (images, questions, labels) in enumerate(train_loader):\n",
        "            images = images.to(device)\n",
        "            questions = questions.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images, questions)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            batch_train_losses.append(loss.item())\n",
        "\n",
        "        train_loss = sum(batch_train_losses) / len(batch_train_losses)\n",
        "        train_losses.append(train_loss)\n",
        "        val_loss, val_acc = evaluate(model, val_loader,criterion, device)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        print(f'EPOCH {epoch + 1}:\\tTrain loss: {train_loss:.4f}\\tVal loss: {val_loss:.4f}\\tVal Acc: {val_acc}')\n",
        "\n",
        "        scheduler.step()\n",
        "    return train_losses, val_losses"
      ],
      "metadata": {
        "id": "49vJfln1rWcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss, optimizer and scheduler"
      ],
      "metadata": {
        "id": "UJnZQVgnreHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses, val_losses = fit(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    device,\n",
        "    epochs\n",
        ")"
      ],
      "metadata": {
        "id": "UuUIzlrmrlrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_loss, val_acc = evaluate(\n",
        "    model,\n",
        "    val_loader,\n",
        "    criterion,\n",
        "    device)\n",
        "test_loss, test_acc = evaluate(\n",
        "    model,\n",
        "    test_loader,\n",
        "    criterion,\n",
        "    device )\n",
        "print('Evaluation on val/test dataset')\n",
        "print('Val accuracy: ', val_acc)\n",
        "print('Test accuracy: ', test_acc)"
      ],
      "metadata": {
        "id": "rgHHNNKWrsys"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}