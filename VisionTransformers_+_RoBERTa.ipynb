{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwcV2tPIzT31x7IoVueR1u"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# VisionTransformer + RoBERTa"
      ],
      "metadata": {
        "id": "86w6UAVBwhcQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "rL3srVbuwn1q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mt69dY7vaZO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import timm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import ViTModel, ViTImageProcessor\n",
        "from transformers import AutoTokenizer, RobertaModel"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train, val, test"
      ],
      "metadata": {
        "id": "f3ZKbuajwxBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load train data\n",
        "train_data = []\n",
        "train_set_path = './vaq2.0.TrainImages.txt'\n",
        "\n",
        "with open(train_set_path, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "        temp = line.split('\\t')\n",
        "        qa = temp[1].split('?')\n",
        "\n",
        "        if len(qa) == 3:\n",
        "            answer = qa[2].strip()\n",
        "        else:\n",
        "            answer = qa[1].strip()\n",
        "\n",
        "        data_sample = {\n",
        "            'image_path': temp[0][:-2],\n",
        "            'question': qa[0] + '?',\n",
        "            'answer': answer\n",
        "        }\n",
        "        train_data.append(data_sample)\n",
        "\n",
        "# Load val data\n",
        "val_data = []\n",
        "val_set_path = './vaq2.0.DevImages.txt'\n",
        "\n",
        "with open(val_set_path, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "        temp = line.split('\\t')\n",
        "        qa = temp[1].split('?')\n",
        "\n",
        "        if len(qa) == 3:\n",
        "            answer = qa[2].strip()\n",
        "        else:\n",
        "            answer = qa[1].strip()\n",
        "\n",
        "        data_sample = {\n",
        "            'image_path': temp[0][:-2],\n",
        "            'question': qa[0] + '?',\n",
        "            'answer': answer\n",
        "        }\n",
        "        val_data.append(data_sample)\n",
        "\n",
        "# Load test data\n",
        "test_data = []\n",
        "test_set_path = './vaq2.0.TestImages.txt'\n",
        "\n",
        "with open(test_set_path, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "        temp = line.split('\\t')\n",
        "        qa = temp[1].split('?')\n",
        "        if len(qa) == 3:\n",
        "            answer = qa[2].strip()\n",
        "        else:\n",
        "            answer = qa[1].strip()\n",
        "\n",
        "        data_sample = {\n",
        "            'image_path': temp[0][:-2],\n",
        "            'question': qa[0] + '?',\n",
        "            'answer': answer\n",
        "        }\n",
        "        test_data.append(data_sample)"
      ],
      "metadata": {
        "id": "p3GgpEnewmj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dictionary mapping classes"
      ],
      "metadata": {
        "id": "IdWmgDVmw7F_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classes = set([sample['answer'] for sample in train_data])\n",
        "classes_to_idx = {\n",
        "    cls_name: idx for idx, cls_name in enumerate(classes)\n",
        "}\n",
        "idx_to_classes = {\n",
        "    idx: cls_name for idx, cls_name in enumerate(classes)\n",
        "}"
      ],
      "metadata": {
        "id": "lLaa3cU0w_Be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class Pytorch Dataset"
      ],
      "metadata": {
        "id": "7KQXNClOxKa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VQADataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data,\n",
        "        classes_to_idx,\n",
        "        img_feature_extractor,\n",
        "        text_tokenizer,\n",
        "        device,\n",
        "        root_dir='/content/val2014-resised/'\n",
        "        ):\n",
        "        self.data = data\n",
        "        self.root_dir = root_dir\n",
        "        self.classes_to_idx = classes_to_idx\n",
        "        self.img_feature_extractor = img_feature_extractor\n",
        "        self.text_tokenizer = text_tokenizer\n",
        "        self.device = device\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = os.path.join(self.root_dir, self.data[index]['image_path'])\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        if self.img_feature_extractor:\n",
        "            img = self.img_feature_extractor(images=img, return_tensors=\"pt\")\n",
        "            img = {k: v.to(self.device).squeeze(0) for k, v in img.items()}\n",
        "\n",
        "        question = self.data[index]['question']\n",
        "        if self.text_tokenizer:\n",
        "            question = self.text_tokenizer(\n",
        "                question,\n",
        "                padding=\"max_length\",\n",
        "                max_length=20,\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "            question = {k: v.to(self.device).squeeze(0) for k, v in question.items()}\n",
        "\n",
        "        label = self.data[index]['answer']\n",
        "        label = torch.tensor(\n",
        "            classes_to_idx[label],\n",
        "            dtype=torch.long\n",
        "            ).to(device)\n",
        "\n",
        "        sample = {\n",
        "            'image': img,\n",
        "            'question': question,\n",
        "            'label': label\n",
        "            }\n",
        "        return sample"
      ],
      "metadata": {
        "id": "y17hF7-IxM2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Objects"
      ],
      "metadata": {
        "id": "hiupxYH4xQFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_feature_extractor = ViTImageProcessor.from_pretrained(\"google/vit-basepatch16-224\")\n",
        "text_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "train_dataset = VQADataset(\n",
        "    train_data,\n",
        "    classes_to_idx=classes_to_idx,\n",
        "    img_feature_extractor=img_feature_extractor,\n",
        "    text_tokenizer=text_tokenizer,\n",
        "    label_encoder=label_encoder,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "val_dataset = VQADataset(\n",
        "    val_data,\n",
        "    classes_to_idx=classes_to_idx,\n",
        "    img_feature_extractor=img_feature_extractor,\n",
        "    text_tokenizer=text_tokenizer,\n",
        "    label_encoder=label_encoder,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "test_dataset = VQADataset(\n",
        "    test_data,\n",
        "    classes_to_idx=classes_to_idx,\n",
        "    img_feature_extractor=img_feature_extractor,\n",
        "    text_tokenizer=text_tokenizer,\n",
        "    label_encoder=label_encoder,\n",
        "    device=device\n",
        ")"
      ],
      "metadata": {
        "id": "j4rN3oWtxRdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "Cq1f9YPzxVU5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Encoder"
      ],
      "metadata": {
        "id": "CRHk2cR6xXWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TextEncoder, self).__init__()\n",
        "        self.model = RobertaModel.from_pretrained(\"roberta-base\")\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        outputs = self.model(**inputs)\n",
        "        return outputs.pooler_output"
      ],
      "metadata": {
        "id": "a_RciPYqxWOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visual Encoder"
      ],
      "metadata": {
        "id": "7uhdCLbYxamn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VisualEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VisualEncoder, self).__init__()\n",
        "        self.model = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n",
        "    def forward(self, inputs):\n",
        "        outputs = self.model(**inputs)\n",
        "        return outputs.pooler_output"
      ],
      "metadata": {
        "id": "A9UMXTPNxdfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classifier"
      ],
      "metadata": {
        "id": "Ue7n1Ca9xhN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=768*2,\n",
        "        hidden_size=512,\n",
        "        n_layers=1,\n",
        "        dropout_prob=0.2,\n",
        "        n_classes=2\n",
        "        ):\n",
        "        super(Classifier,self).__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "        input_size,\n",
        "        hidden_size,\n",
        "        num_layers=n_layers,\n",
        "        batch_first=True,\n",
        "        bidirectional=True\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.fc1 = nn.Linear(hidden_size*2, n_classes)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc1(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "Tm_pFGpTxiu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VQA Model"
      ],
      "metadata": {
        "id": "ISqPP3n5xk95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VQAModel(nn.Module):\n",
        "    def __init__(\n",
        "    self,\n",
        "    visual_encoder,\n",
        "    text_encoder,\n",
        "    classifier\n",
        "    ):\n",
        "\n",
        "        super(VQAModel, self).__init__()\n",
        "        self.visual_encoder = visual_encoder\n",
        "        self.text_encoder = text_encoder\n",
        "        self.classifier = classifier\n",
        "\n",
        "    def forward(self, image, answer):\n",
        "        text_out = self.text_encoder(answer)\n",
        "        image_out = self.visual_encoder(image)\n",
        "        x = torch.cat((text_out, image_out), dim=1)\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def freeze(self, visual=True, textual=True, clas=False):\n",
        "        if visual:\n",
        "            for n,p in self.visual_encoder.named_parameters():\n",
        "                p.requires_grad = False\n",
        "        if textual:\n",
        "            for n,p in self.text_encoder.named_parameters():\n",
        "                p.requires_grad = False\n",
        "        if clas:\n",
        "            for n,p in self.classifier.named_parameters():\n",
        "                p.requires_grad = False"
      ],
      "metadata": {
        "id": "_VhlhIdexm1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_classes = len(classes)\n",
        "hidden_size = 1024\n",
        "n_layers = 1\n",
        "dropout_prob = 0.2\n",
        "\n",
        "text_encoder = TextEncoder().to(device)\n",
        "visual_encoder = VisualEncoder().to(device)\n",
        "classifier = Classifier(\n",
        "    hidden_size=hidden_size,\n",
        "    n_layers=n_layers,\n",
        "    dropout_prob=dropout_prob,\n",
        "    n_classes=n_classes\n",
        "    ).to(device)\n",
        "\n",
        "model = VQAModel(\n",
        "    visual_encoder=visual_encoder,\n",
        "    text_encoder=text_encoder,\n",
        "    classifier=classifier\n",
        "    ).to(device)\n",
        "model.freeze()"
      ],
      "metadata": {
        "id": "hJ7qZNRWxsMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and Evaluate"
      ],
      "metadata": {
        "id": "GU1vji1zxu-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for idx, inputs in enumerate(dataloader):\n",
        "            images = inputs['image']\n",
        "            questions = inputs['question']\n",
        "            labels = inputs['label']\n",
        "            outputs = model(images, questions)\n",
        "            loss = criterion(outputs, labels)\n",
        "            losses.append(loss.item())\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    loss = sum(losses) / len(losses)\n",
        "    acc = correct / total\n",
        "    return loss, acc\n",
        "def fit(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    epochs\n",
        "    ):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        batch_train_losses = []\n",
        "\n",
        "        model.train()\n",
        "        for idx, inputs in enumerate(train_loader):\n",
        "            images = inputs['image']\n",
        "            questions = inputs['question']\n",
        "            labels = inputs['label']\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images, questions)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            batch_train_losses.append(loss.item())\n",
        "\n",
        "        train_loss = sum(batch_train_losses) / len(batch_train_losses)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        val_loss, val_acc = evaluate(\n",
        "            model, val_loader,\n",
        "            criterion\n",
        "        )\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        print(f’EPOCH {epoch + 1}:\\tTrain loss: {train_loss:.4f}\\tVal loss: {val_loss:.4f}\\tVal Acc: {val_acc}’)\n",
        "        scheduler.step()\n",
        "    return train_losses, val_losses"
      ],
      "metadata": {
        "id": "jgmsR9EpxwrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss, optimizer and scheduler"
      ],
      "metadata": {
        "id": "jvfAIyGAx1eT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-2\n",
        "epochs = 50\n",
        "scheduler_step_size = epochs * 0.6\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=lr\n",
        ")\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "    optimizer,\n",
        "    step_size=scheduler_step_size,\n",
        "    gamma=0.1\n",
        ")"
      ],
      "metadata": {
        "id": "wPGY07w1x5D1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "CPSCJI53x9D-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses, val_losses = fit(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    epochs\n",
        ")"
      ],
      "metadata": {
        "id": "asuGyBkVx-Qp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "XRI4_0w8yAsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_loss, val_acc = evaluate(\n",
        "    model,\n",
        "    val_loader,\n",
        "    criterion\n",
        ")\n",
        "test_loss, test_acc = evaluate(\n",
        "    model,\n",
        "    test_loader,\n",
        "    criterion\n",
        ")\n",
        "\n",
        "print('Evaluation on val/test dataset')\n",
        "print('Val accuracy: ', val_acc)\n",
        "print('Test accuracy: ', test_acc)"
      ],
      "metadata": {
        "id": "RVxoS6cFyBz3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}